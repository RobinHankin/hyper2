% -*- mode: noweb; noweb-default-code-mode: R-mode; -*-
\documentclass[article]{jss}
\usepackage{booktabs}
\usepackage{units}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{wrapfig}

%  Process with:
%  
%  Rscript -e 'library("knitr") ; knit("jss_hyper3_real.Rnw")' && pdflatex jss_hyper3_real && open jss_hyper3_real.pdf
%
%  (also the Makefile has an entry for this)

\author{Robin K. S. Hankin\\University of Stirling}
\title{Generalized Bradley-Terry models}
\Plainauthor{Robin K. S. Hankin} %% comma-separated
\Plaintitle{Generalized Bradley-Terry models}

%% an abstract and keywords
\Abstract{ 
  
  A generalization of \code{hyper2} is presented in which
  Bradley-Terry strengths may be modified using a multiplicative
  factor $\lambda$; Generally, ordinary Bradley-Terry is recovered if
  $\lambda=1$.  This approach offers multiple advantages over other
  generalizations such as reified Bradley-Terry.  It becomes possible
  to represent non-independence of successive observations in
  Plackett-Luce likelihood functions which allows a parametrization of
  the ``red bus, blue bus'' problem in preference analysis.  I present
  examples drawn from the fields of food preference and the e-sport
  {\em Counterstrike}.  }

\Keywords{Plackett-Luce, Bradley-Terry, Mann-Whitney}
\Plainkeywords{Plackett-Luce, Bradley-Terry, Mann-Whitney}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Robin K. S. Hankin\\
  University of Stirling\\
  E-mail: \email{hankin.robin@gmail.com}\\
  URL: \url{https://academics.aut.ac.nz/robin.hankin}
}

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\headercell[1]{%
   \smash[b]{\begin{tabular}[t]{@{}c@{}} #1 \end{tabular}}}
\newcommand{\draw}{\ensuremath{\mathfrak{D}}}
\newcommand{\cdraw}{\ensuremath{\mathfrak{C}}}
\newcommand{\white}{\ensuremath{\mathfrak{W}}}
\newcommand{\home}{\ensuremath{\mathfrak{H}}}
\newcommand{\oo}{\vphantom{W^{W^{W^W}}}}

\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
source("loadlibrary.R",echo=FALSE)  # to avoid stupid messages
opts_chunk$set(cache=TRUE, autodep=TRUE)
options(prompt = "R> ")
@

\section{Introduction}


\begin{wrapfigure}{r}{0.1\textwidth}
    \centering
    \includegraphics[width=0.1\textwidth]{hyper2.png}\\
\end{wrapfigure}

The Bradley-Terry model for datasets involving paired comparisons has
wide uptake in the {R} community.  The canonical problem is to
consider~$n$ players who compete against one another; the basic
inference problem is to estimate
numbers~$\mathbf{p}=\left({p_1,\ldots,p_n}\right)$, $p_i\geqslant 0$,
$\sum p_i=1$ so that
$\Prob(\mbox{$i$~beats~$j$})=\frac{p_i}{p_i+p_j}$.  Information about
the~$p_i$ may be obtained from the results of paired comparisons
between the players.

One generalization of the Bradley-Terry model is due
to~\citet{luce1959}, in which the probability of competitor~$i$
winning in a field of~$j=1,\ldots, n$ is $\frac{p_i}{p_1+\cdots
  +p_n}$.  It is then natural to quantify some ephemeral enhancement
to competitor~$i$ using a multiplicative factor $\lambda$.  Then
$\Prob(\mbox{$i$ wins})=\frac{\lambda p_i}{p_1+\cdots+\lambda
  p_i+\cdots+p_n}$.  Examples of such enhancements include the
home-ground advantage in football and rugby, or playing white in
chess~\citep{hankin2020}.  Note that $\lambda < 1$ would correspond to
a decrease in competitive ability.

Further, one might follow~\cite{hankin2017_rnw} and consider teams of
players.  In doubles tennis, for example, $\left\lbrace
p_1,p_2\right\rbrace$ might play $\left\lbrace p_3,p_4\right\rbrace$
and win with probability
$\frac{p_1+p_2}{p_1+p_2+p_3+p_4}$~\citep{hankin2010}.  It is natural
to combine this idea with that of a multiplicative factor.  For
example, suppose $p_1$ has a special move against $p_3$, and $p_4$ has
just been given a performance-enhancing drug.  If so, we might have
likelihood function

\begin{equation}\label{like1234}
  \mathcal{L}{\left(p_1,p_2,p_3,p_4;\lambda,\mu\right)}\propto
  \frac{
    \left(\lambda p_1 + p_2\right)^{w_{12}}\cdot
    \left(p_3 + \mu p_4\right)^{w_{34}}
    }{
    \left(\lambda p_1+p_2+p_3+\mu p_4\right)^{w_{12} + w_{34}}
    }
\end{equation}


where $w_{12}$ and $w_{34}$ are the number of wins by each team
respectively.  Then $\lambda$ and $\mu$ represent the effectiveness of
$p_1$'s special move and the enhancement afforded to $p_4$ by the
drug, with values $>1$ corresponding to a positive effect.  In this
paper I generalize the {\tt hyper2} suite for Bradley-Terry
likelihoods~\citep{hankin2017_rnw} to accommodate this class of
multiplicative factors.  The most general likelihood function would be
proportional to

\begin{equation}
  \prod_{i=1}^N
  \sum_{j=1}^n
 \left(\lambda_{ij}p_j\right)^{w_i}
  \end{equation}

where the $p_j$ are traditional Bradley-Terry strengths, $p_j\geqslant
0$, $\sum p_j=1$, and the $\lambda_{ij}\geqslant 0$ are multiplicative
factors that account for ephemeral or temporary changes in competitive
ability.  The data $w_1,\ldots w_N$ correspond to the number of
``wins'' and number of matches, as per $w_{12},w_{34}$ in
equation~\ref{like1234}.  In practice, almost all the $\lambda_{ij}$
are zero.  To fix ideas, consider the data presented in
Table~\ref{oldfirm}.  Ignoring draws for the moment, we have a
likelihood proportional to

\begin{table}
\centering
\begin{tabular}{@{} *{5}{c} @{}}
\headercell{\small{Plays at home,}\\ \small{wins}} &
\multicolumn{4}{c@{}}{\small{Plays away, loses}}\\
\cmidrule(l){2-5}
&   Rangers     &    Celtic & Falkirk & Livingston\\
\midrule
   Rangers      &   NA &    81  &    69   &      15\\
   Celtic       &   76 &    NA  &    61   &      15\\
   Falkirk      &   17 &    17  &    NA   &       6\\
   Livingston   &    2 &     2  &     3   &      NA\\
\end{tabular}
\\
\rule{1mm}{6mm}
\\
\begin{tabular}{@{} *{5}{c} @{}}
\headercell{\small{Plays at home,}\\ \small{draws}} &
\multicolumn{4}{c@{}}{\small{Plays away, draws}}\\
\cmidrule(l){2-5}
&  Rangers     &    Celtic & Falkirk & Livingston\\
\midrule
   Rangers     &       NA  &   44 &     14   &       2\\
   Celtic      &       47  &   NA &     10   &       3\\
   Falkirk     &       12  &   19 &     NA   &       4\\
   Livingston  &       4   &   4  &     4    &      NA\\
   \end{tabular}
\\
\rule{1mm}{6mm}
\\
\begin{tabular}{@{} *{5}{c} @{}}
\headercell{\small{Plays at home,}\\ \small{loses}} &
\multicolumn{4}{c@{}}{\small{Plays away, wins}}\\
\cmidrule(l){2-5}
&  Rangers     &    Celtic & Falkirk & Livingston\\
\midrule
   Rangers     &   NA  &  42  &    2  &       0\\
   Celtic      &   46  &  NA  &    8  &       0\\
   Falkirk     &   57  &  43  &   NA  &       8\\
   Livingston  &   12  &  11  &    9  &      NA\\
  \end{tabular}
\caption{The \label{oldfirm} dominance of the Old Firm: 669 matches
  between Rangers, Celtic, Falkirk and Livingston arranged by home
  wins, draws, and home losses.  Data from {\tt soccerworld}}
\end{table}

\begin{equation}\label{firstfewRCFL}
  \left(\frac{\lambda p_R}{\lambda p_R + p_C}\right)^{81}\cdot
  \left(\frac{\lambda p_R}{\lambda p_R + p_F}\right)^{69}\cdots
  \left(\frac{\lambda p_L}{\lambda p_L + p_F}\right)^{81}\cdots
  \left(\frac{p_C}{p_C + \lambda p_R}\right)^{42}\cdots
    \left(\frac{p_F}{p_F + \lambda p_L}\right)^{9}
\end{equation}

where the home-ground advantage is represented by~$\lambda$.  In {\tt
  hyper3} idiom, a term such as $\lambda p_R + p_C$ is represented as
a named numeric vector.  For example, if the home strength were
$\lambda = 1.8$ then the first bracket would be {\tt c(Rangers=1.8,
  Celtic=1)} with an associated power of $-81$.  The computational
methodology is a generalization of that used
in~\cite{hankin2024_hyper3} in which the multiplicative factors
correspond to number of clones.  However, in~\cite{hankin2024_hyper3}
the factors are known non-negative integers, and here they are, in
general, unknown non-negative real numbers.

\subsection{The package in use}
Package idiom for creating a likelihood function for this dataset is
straightforward.  Considering first only Rangers-Celtic matches:

<<defineoldfirm, cache=TRUE>>=
oldfirm <- function(lambda){
    H <- hyper3(pnames = c("Rangers", "Celtic"))
    H[c(Rangers = lambda            )] %<>%      add(81     )
    H[c(                  Celtic = 1)] %<>%      add(     42)
    H[c(Rangers = lambda, Celtic = 1)] %<>% subtract(81 + 42)

    H[c(Celtic = lambda             )] %<>%      add(     76)
    H[c(                 Rangers = 1)] %<>%      add(46     )
    H[c(Celtic = lambda, Rangers = 1)] %<>% subtract(46 + 76)
    return(H)
}
@

Above we see package idiom for creating a log-likelihood function.  We
first create an empty {\tt hyper3} object with an appropriate names
attribute.  The next three lines, using \pkg{magrittr} assignment
pipes, give the results for Rangers at home, and the next three lines
give the result for Celtic at home.  It is then easy to create a
bespoke log-likelihood function for Rangers-Celtic matches:

<<useoldfirm, cache = TRUE>>=
oldfirm(1.88)
@


We see a log-likelihood function for all $82+42+76+46=245$
Rangers-Celtic matches that were not score draws, conditional on
assumed value $\lambda=1.88$.  This corresponds to the relevant terms
of equation~\ref{firstfewRCFL}.  We may make inferences about the true
value of $\lambda$ using profile support:

<<profilelikeoldfirm, cache=TRUE>>=
f <- function(lambda){maxp(oldfirm(lambda),give=1)$`Log-likelihood`}
lam <- seq(from=0.9, to=3, len=23)
logLike <- sapply(lam,f)
@

Thus {\tt f()} returns the support at the evaluate, conditional on a
supplied value of $\lambda$.  We may use this to create a profile
likelihood (figure~\ref{profilesupportlambda}), or to find the maximum
likelihood estimate for $\lambda$:

\begin{figure}[htbp]
  \begin{center}
<<plotproflikeoldfirm, echo=FALSE>>=
plot(lam,logLike - max(logLike),type='b')
abline(v=1,col='gray')
segments(x0=1.785,y0=0.1,y1=-1,col='red')
abline(h = c(0,-2),col='gray')
@
\caption{A\label{profilesupportlambda} profile support function for
  the home-team advantage $\lambda$.  Vertical gray line shows
  $H_0\colon\lambda =1$, horizontal lines show the
  two-units-of-support limits; small red bar shows
  $\hat{\lambda}\simeq 1.785$}
\end{center}
\end{figure}


<<optimoldfirm, cache=TRUE>>=
(lambda_max <-  optimize(f,c(1.5,2.0),maximum=TRUE)$maximum)
@

Then using this value we may estimate the Bradley-Terry strengths of
Rangers and Celtic:

<<mleoldfirm,cache=TRUE>>=
maxp(oldfirm(lambda_max))
@

Compare with the theoretical value of $\widehat{p_{\mbox{\tiny
      C}}}=\frac{\sqrt{42\cdot 76}}{\sqrt{81\cdot 46}+\sqrt{42\cdot
    76}}\simeq 0.4807$, $\widehat{p_{\mbox{\tiny R}}}
=\frac{\sqrt{81\cdot 46}}{\sqrt{81\cdot 46}+\sqrt{42\cdot 76}}\simeq
0.5193$, and $\widehat{\lambda}=\frac{\sqrt{76\cdot 81}}{\sqrt{46\cdot
    42}}\simeq 1.785$.


\section{Score draws}

About 22\% of matches in Table~\ref{oldfirm} end in a draw and {\tt
  hyper2} formalism may be used to assess this.  Consider the
likelihood function described in Table~\ref{windrawloss}; this has a
number of desirable features not present in the reified entity
approach of~\citet{hankin2020}.  Firstly, if we consider two players
of identical strengths and nullify the home ground effect (that is,
$\lambda=1$) then the probability of a draw is $\frac{2D}{2+2D}$; the
odds of a draw is simply $D$, regardless of team strength.  Secondly,
considering only won (that is, not score-drawn) games, the probability
of a home win is $\frac{\lambda}{1+\lambda}$, that is, the odds of a
home win is simply $\lambda$, independently of the player
strength. Such arguments give direct operational significance to
$\lambda$ and $D$.




\begin{table}
\centering
\begin{tabular}{l|c|c}\\&
{$i$ plays at home} &
{$i$ plays away} \\
  \hline
  $\Prob{(\mbox{$i$ wins})}$                             &
  $\frac{\lambda p_i}{\lambda p_i + D(p_i + p_j) + p_j}$  &
  $\frac{p_i}{p_i + D(p_i + \lambda p_j) + p_j}$          \\
  $\Prob{(\mbox{draw})}$                                 &
  $\frac{D(p_i + p_j)}{\lambda p_i+D(p_i+p_j) + p_j}$     &
  $\frac{D(p_i + p_j)}{p_i + D(p_i+p_j) + \lambda p_j}$   \\
  $\Prob{(\mbox{$i$ loses})}$                            &
  $\frac{p_j}{\lambda p_i + D(p_i+p_j) + p_j}$            &
  $\frac{\lambda p_j}{p_i + D(p_i + p_j) + \lambda p_j}$  \\
\end{tabular}
\caption{Probabilities of a win, draw, loss for the two teams under a
  two-parameter multiplicative Bradley-Terry model\label{windrawloss}}
\end{table}

We are now in a position to consider the entirety of
Table~\ref{oldfirm}.  Bespoke package function
\code{home\_draw\_away3()} converts data in the form of
Table~\ref{oldfirm} to a \code{hyper3} likelihood function following
Table~\ref{windrawloss}; it takes numerical values for the home
advantage $\lambda$ and proclivity to draw $D$:


<<eg_RCLF3_lf, cache=TRUE>>=
home_draw_away3(RCLF3_table, lambda=1.7, D=0.3)
@

Observe that the resulting likelihood function has two distinct types
of parameters: the plain Bradley-Terry strengths $\mathbf{p}=(p_R,
p_C,p_L,p_F)$, with unit sum; but also the auxiliary parameters
$\lambda,D$.  To make inferences about these parameters we calculate a
profile likelihood function: conditional on $\lambda,D$ we maximize
the likelihood over possible values of $\mathbf{p}$.


<<defineF>>=
f <- function(v){
    H <- home_draw_away3(RCLF3_table, lambda = v[1], D=v[2])
    maxp(H,give=1)$`Log-likelihood`
}
@

Thus \code{f()} gives us a likelihood function for
$\mathbf{v}=(\lambda,D)$.  We may find the evaluate using
\code{optim()}:

<<findmaxlambdaD, cache=TRUE>>=
maxv <- optim(par=c(2, 0.3), fn=f, control=list(fnscale = -1))$par
maxv
@

Then

<<showevaluate, cache=TRUE>>=
maxp(home_draw_away3(RCLF3_table, lambda = maxv[1], D=maxv[2]))
@

Such estimates have rich predictive power.  For example, if Rangers
were to play Celtic with both teams away [thus nullifying the home
  ground advantage: $\lambda=1$ in Table~\ref{windrawloss}], the
probability of win/draw/loss for Rangers would be
$\left(\frac{p_R}{(1+D)(p_R+p_C)};\frac{D}{1+D};\frac{p_C}{(1+D)(p_R+p_C)}\right)$,
estimated at $(0.39;0.27;0.34)$.


<<makecontourdata, echo=FALSE, cache=TRUE>>=
n <- 20
lambda <- seq(from=1, to=3, len=n)
D <- seq(from=0.1, to=0.6, len=n)
V <- as.matrix(expand.grid(lambda, D))
LL <- apply(V, 1, f)
LL <- pmax(-20, LL - max(LL))
@ 


\begin{figure}[htbp]
  \begin{center}
<<docontourplot, cache=FALSE>>=
contour(lambda, D, matrix(LL, n, n),levels = -2*(1:9), xlab=expression(lambda),ylab=expression(D))
points(maxv[1], maxv[2], pch=16, col='red')
@
\caption{Contours of support\label{contourlambdaD} evaluated over
  a grid.  Maximum likelihood estimate for $(\lambda, D)$ shown as
  a red dot at the evaluate}
  \end{center}
\end{figure}


<<testlambdazero, cache=TRUE, include=FALSE>>=
jj <- optimize(f=function(D){f(c(lambda=1,D))}, interval=c(0.1, 0.4), maximum=TRUE)
@

Further, we may calculate a support function over the
$(\lambda,D)$-plane, shown in figure~\ref{contourlambdaD}.  For
example, we may test the hypothesis that $\lambda=1$ [corresponding to
  the right hand side of figure~\ref{contourlambdaD}] for which the
support is about \Sexpr{round(f(maxv) - jj$objective,2)}, far
exceeding the two units of support per degree of freedom
criteron~\citep{edwards1992}.  Alternatively, we can observe that this
lies in the tail region of its asymptotic null $\chi^2_4$ distribution
with a $p$-value of about \Sexpr{signif(pchisq(f(maxv) -
  jj$objective,lower.tail=FALSE,df=2),digits=2)}.


\section{Non-independence in successive arrivals}

In the following, I use the ``race'' metaphor: five runners take part
in a race and arrive in order $a\succ b\succ c\succ d\succ e$.  Thus
competitor $a$ wins, $b$ comes second, and so on.  Consider the
following Plackett-Luce likelihood function for this observation:

\begin{equation}
\frac{p_a}{p_a + p_b + p_c + p_d + p_e}\cdot
\frac{p_b}{      p_b + p_c + p_d + p_e}\cdot
\frac{p_c}{            p_c + p_d + p_e}\cdot
\frac{p_d}{                  p_d + p_e}\cdot
\frac{p_e}{                        p_e}
\end{equation}

Note the use of multiplication between the terms, indicating
conditional independence.  Now we use the multiplicative
generalization of Bradley-Terry strengths to introduce an element of
non-independence between the terms.  There are many ways to do this,
but one simple way would be to define equivalence classes of the
competitors with each equivalence class comprising mutually supporting
runners.  The metaphor would be that a runner who has finished the
race is able to support other members of his equivalence class by
cheering his still-running teammates, boosting their performance.  The
\pkg{hyper2} package includes function \code{cheering()} which
implements this functionality.

As an example, consider figure~\ref{tikzabcde}.  This shows a partial
probability tree diagram for some of the $5!=120$ possible order
statistics.  The standard Plackett-Luce likelihoods have been modified
to account for two groups of mutually supporting runners:
$\left\lbrace a,b,c\right\rbrace$ with support term $\lambda$, and
$\left\lbrace d,e\right\rbrace$ with support term $\mu$.  From START,
the first runner to cross the finishing line has standard
Plackett-Luce probability.  Taking the top path as an example, we see
that the likelihood function for $a\succ b\succ c\succ d\succ e$ would
be

\begin{figure}[htbp]
  \begin{center}
\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm]
\fill (0,0) circle[radius=2pt]; % root; paths abcde
\draw[thick] (0,0) -- (2, 3) -- (4.8, 3);
\draw[thick] (0,0) -- (2, 2) -- (4.8, 2);
\draw[thick] (0,0) -- (2, 1) -- (4.8, 1);
\draw[thick] (0,0) -- (2, 0) -- (4.8, 0);
\draw[thick] (0,0) -- (2,-1) -- (4.8,-1);
\node at (0,2.5) (eq1) {$\underbrace{\left\lbrace a,b,c\right\rbrace}_{\lambda}\underbrace{\left\lbrace d,e\right\rbrace}_{\mu}$};

\node at (4.8, 2.8) {$a$};
\node at (4.8, 1.8) {$b$};
\node at (4.8, 0.8) {$c$};
\node at (4.8,-0.2) {$d$};
\node at (4.8,-1.2) {$e$};

\node at (3.4, 3.4)  {$\frac{p_a}{p_a+p_b+p_c+p_d+p_e}$};
\node at (3.4, 2.4)  {$\frac{p_b}{p_a+p_b+p_c+p_d+p_e}$};
\node at (3.4, 1.4)  {$\frac{p_c}{p_a+p_b+p_c+p_d+p_e}$};
\node at (3.4, 0.4)  {$\frac{p_d}{p_a+p_b+p_c+p_d+p_e}$};
\node at (3.4,-0.6)  {$\frac{p_e}{p_a+p_b+p_c+p_d+p_e}$};

\fill (4.8, 3) circle[radius=2pt];  % a finishes; paths bcde
\fill (4.8, 2) circle[radius=2pt];  % terminal node
\fill (4.8, 1) circle[radius=2pt];  % terminal node
\fill (4.8, 0) circle[radius=2pt];  % d finishes; paths abce
\fill (4.8,-1) circle[radius=2pt];  % terminal node

\draw[thick] (4.8,3) -- (5.8,5) -- (8.2,5);
\draw[thick] (4.8,3) -- (5.8,4) -- (8.2,4);
\draw[thick] (4.8,3) -- (5.8,3) -- (8.2,3);
\draw[thick] (4.8,3) -- (5.8,2) -- (8.2,2);

\newcommand{\la}[1]{\lambda{#1}}
\newcommand{\ld}[1]{\mu    {#1}}
\node at (7, 5.4)  {$\frac{\la{p_b}}{\la{p_b}+\la{p_c}+p_d+p_e}$};
\node at (7, 4.4)  {$\frac{\la{p_c}}{\la{p_b}+\la{p_c}+p_d+p_e}$};
\node at (7, 3.4)  {$\frac{p_d     }{\la{p_b}+\la{p_c}+p_d+p_e}$};
\node at (7, 2.4)  {$\frac{p_e     }{\la{p_b}+\la{p_c}+p_d+p_e}$};

\fill (8.2,5) circle[radius=2pt];  % ab finishes; paths cde
\fill (8.2,4) circle[radius=2pt];  % ab finishes; paths cde
\fill (8.2,3) circle[radius=2pt];  % ab finishes; paths cde
\fill (8.2,2) circle[radius=2pt];  % ab finishes; paths cde

\node at (8.2,4.8) {$b$};
\node at (8.2,3.8) {$c$};
\node at (8.2,2.8) {$d$};
\node at (8.2,1.8) {$e$};

\draw[thick] (8.2,5) -- (8.9,6) -- (10.6,6);
\draw[thick] (8.2,5) -- (8.9,5) -- (10.6,5);
\draw[thick] (8.2,5) -- (8.9,4) -- (10.6,4);



\node at (9.5, 6.4) {$\frac{\la{p_c}}{\la{p_c}+p_d+p_e}$};
\node at (9.5, 5.4) {$\frac{p_d  }{\la{p_c}+p_d+p_e}$};
\node at (9.5, 4.4) {$\frac{p_e  }{\la{p_c}+p_d+p_e}$};

\fill (10.6, 6) circle[radius=2pt];  % abc finishes; paths de
\node at (10.6,5.8) {$c$};
\draw[thick] (10.6, 6) -- (11.2,7) -- (12.2,7);
\fill (12.2,7) circle[radius=2pt];
\node[anchor = west] at (12.2,7) {$a\succ b\succ c\succ d\succ e$};
\node at (11.8, 7.4) {$\frac{p_d}{p_d+p_e}$};

\draw[thick] (10.6, 6) -- (11.2,6.2) -- (12.2,6.2);
\fill (12.2,6.2) circle[radius=2pt];
\node[anchor = west] at (12.2,6.2) {$a\succ b\succ c\succ e\succ d$};
\node at (11.8, 6.6) {$\frac{p_e}{p_d+p_e}$};

\fill (10.6, 5) circle[radius=2pt];
\node at (10.6,4.8) {$d$};

\draw[thick] (10.6, 5) -- (11.2,5.3) -- (12.2, 5.3);
\fill (12.2,5.3) circle[radius=2pt];
\node[anchor = west] at (12.2,5.3) {$a\succ b\succ d\succ c\succ e$};
\node at (11.8, 5.7) {$\frac{\la{p_c}}{\la{p_c}+\ld{p_e}}$};

\draw[thick] (10.6, 5) -- (11.2,4.7) -- (12.2, 4.7);
\fill (12.2,4.7) circle[radius=2pt];
\node[anchor = west] at (12.2,4.7) {$a\succ b\succ d\succ e\succ c$};
\node at (11.8, 5.0) {$\frac{\ld{p_e}}{\la{p_c}+\ld{p_e}}$};

\fill (10.6, 4) circle[radius=2pt];
\node at (10.6,3.8) {$e$};
\draw[thick] (10.6, 4) -- (11.2,3.9) -- (12.2, 3.9);
\fill (12.2,3.9) circle[radius=2pt];
\node[anchor = west] at (12.2,3.9) {$a\succ b\succ e\succ c\succ d$};
\node at (11.8, 4.3) {$\frac{\la{p_c}}{\la{p_c}+\ld{p_d}}$};

\draw[thick] (10.6, 4) -- (11.2,3.1) -- (12.2,3.1);
\fill (12.2,3.1) circle[radius=2pt];
\node at (11.8, 3.5) {$\frac{\ld{p_d}}{\la{p_c}+\ld{p_d}}$};
\node[anchor = west] at (12.2,3.1) {$a\succ b\succ e\succ d\succ c$};


\draw[thick] (4.8, 0) -- (5.8,1) -- (8.2,1);
\fill (12.2,1) circle[radius=2pt];
\node[anchor = west] at (12.2,1) {$d\succ a\succ c\succ e\succ b$};
\node at (7, 1.4)  {$\frac{p_a}{p_a+p_b+p_c+\ld{p_e}}$};

\draw[thick] (4.8, 0) -- (5.8,0) -- (8.2,0);
\fill (12.2,0) circle[radius=2pt];
\node[anchor = west] at (12.2,0) {$d\succ e\succ a\succ b\succ c$};
\node at (7, 0.4)  {$\frac{p_b}{p_a+p_b+p_c+\ld{p_e}}$};

\draw[thick] (4.8, 0) -- (5.8,-1) -- (8.2,-1);
\node at (7, -0.6)  {$\frac{p_c}{p_a+p_b+p_c+\ld{p_e}}$};

\draw[thick] (4.8, 0) -- (5.8,-2) -- (8.2,-2);
\node at (7, -1.6)  {$\frac{\ld{p_e}}{p_a+p_b+p_c+\ld{p_e}}$};

\fill (8.2, -2) circle[radius=2pt];  % de finishes; paths abc
\fill (8.2, -1) circle[radius=2pt];  % de finishes; paths abc
\fill (8.2,  0) circle[radius=2pt];  % de finishes; paths abc

\node at (8.2, -2.2) {$e$};
\node at (8.2, -1.2) {$c$};
\node at (8.2, -0.2) {$b$};
\node at (8.2,  0.8) {$a$};

\draw[thick] (8.2, -2) -- (8.9,-1) -- (10.6,-1);
\draw[thick] (8.2, -2) -- (8.9,-2) -- (10.6,-2);
\draw[thick] (8.2, -2) -- (8.9,-3) -- (10.6,-3);

\fill (12.2,-1) circle[radius=2pt];
\node[anchor = west] at (12.2,-1) {$d\succ e\succ a\succ c\succ b$};

\node at (9.5, -0.6) {$\frac{p_a}{p_a+p_b+p_c}$};
\node at (9.5, -1.6) {$\frac{p_b}{p_a+p_b+p_c}$};
\node at (9.5, -2.6) {$\frac{p_c}{p_a+p_b+p_c}$};

\fill (10.6, -1) circle[radius=2pt];  % dea finishes; paths bc
\fill (10.6, -2) circle[radius=2pt];  % deb finishes; paths ac
\fill (10.6, -3) circle[radius=2pt];  % dec finishes; paths ab
\node at (10.6,-1.2) {$a$};
\node at (10.6,-2.2) {$b$};
\node at (10.6,-3.2) {$c$};


\draw[thick] (10.6, -1) -- (11.2,-0) -- (12.2,0);

\node at (11.8, 0.4) {$\frac{\la{p_b}}{\la{p_b}+\la{p_c}}$};
\draw[thick] (10.6, -1) -- (11.2,-1) -- (12.2,-1);

\node at (11.8, -0.6) {$\frac{\la{p_c}}{\la{p_b}+\la{p_c}}$};

\draw[thick](10.6, -2) -- (11.2,-1.8) -- (12.2,-1.8);
\fill (12.2,-1.8) circle[radius=2pt];
\node[anchor = west] at (12.2,-1.8) {$d\succ e\succ b\succ a\succ c$};
\node at (11.8, -1.4) {$\frac{\la{p_a}}{\la{p_a}+\la{p_c}}$};

\draw[thick] (10.6, -2) -- (11.2,-2.6) -- (12.2,-2.6);
\fill (12.2,-2.6) circle[radius=2pt];
\node[anchor = west] at (12.2,-2.6) {$d\succ e\succ b\succ c\succ a$};
\node at (11.8, -2.2) {$\frac{\la{p_c}}{\la{p_a}+\la{p_c}}$};


%\draw[thick] (10.6, -3) -- (11.2,-2.9);
%\fill (12.2,-2.9) circle[radius=2pt];
%\node at (11.8, -2.9) {$\frac{\la{p_a}}{\la{p_a}+\la{p_b}}$};

\draw[thick] (10.6, -3) -- (11.2,-3.4) -- (12.2, -3.4);
\fill (12.2,-3.4) circle[radius=2pt];
\node[anchor = west] at (12.2,-3.4) {$d\succ e\succ c\succ b\succ a$};

\node at (11.8, -3) {$\frac{\la{p_b}}{\la{p_a}+\la{p_b}}$};

\fill (8.2, 1) circle[radius=2pt];  % da finishes; paths bce

\draw[thick] (8.2,1) -- (8.9,2) -- (10.6,2);
\draw[thick] (8.2,1) -- (8.9,1) -- (10.6,1);
\draw[thick] (8.2,1) -- (8.9,0) -- (10.6,0);

\node at (9.5, 2.4) {$\frac{\la{p_b}}{\la{p_b}+\la{p_c}+p_e}$};
\node at (9.5, 1.4) {$\frac{\la{p_c}}{\la{p_b}+\la{p_c}+p_e}$};
\node at (9.5, 0.4) {$\frac{p_e     }{\la{p_b}+\la{p_c}+p_e}$};
\fill (10.6, 2) circle[radius=2pt];  % da finishes; paths bce
\fill (10.6, 1) circle[radius=2pt];  % da finishes; paths bce
\fill (10.6, 0) circle[radius=2pt];  % da finishes; paths bce

\node at (10.6, 1.8) {$b$};
\node at (10.6, 0.8) {$c$};
\node at (10.6,-0.2) {$e$};


\draw[thick] (10.6, 1) -- (11.2,2) -- (12.2,2);
\node at (11.8, 2.4) {$\frac{\la{p_b}}{\la{p_b}+\ld{p_e}}$};
\fill (12.2,2) circle[radius=2pt];
\node[anchor = west] at (12.2,2) {$d\succ a\succ c\succ b\succ e$};
\draw[thick] (10.6, 1) -- (11.2,1) -- (12.2,1);
\node at (11.8, 1.4) {$\frac{\ld{p_e}}{\la{p_b}+p_e}$};

\end{tikzpicture}
\caption{Partial probability\label{tikzabcde} tree for five
  competitors $a$-$e$ with two mutually supporting subsets
  $\left\lbrace a,b,c\right\rbrace$ and $\left\lbrace d,
  e\right\rbrace$ having support $\lambda$, $\mu$ respectively}
\end{center}
\end{figure}


\begin{equation}\label{asuccb}
\frac{p_a}{p_a + p_b + p_c + p_d + p_e}\cdot
\frac{\lambda p_b}{\lambda p_b + \lambda p_c + p_d + p_e}\cdot
\frac{\lambda p_c}{\lambda p_c + p_d + p_e}\cdot
\frac{p_d}{p_d + p_e}
\end{equation}

In equation \ref{asuccb}, the first term is standard Plackett-Luce: at
this point, no competitor has finished and cheering effects are
absent.  The second term reflects the fact that competitors $b$ and
$c$ are supported by competitor $a$, who has by this point finished
the race and is supporting his teammates.

By contrast, the likelihood function for observation $d\succ a\succ
c\succ b\succ e$ would be

\begin{equation}
\frac{p_d}{p_a + p_b + p_c + p_d + p_e}\cdot
\frac{p_a}{p_a + p_b + p_c + \mu p_e}\cdot
\frac{\lambda p_c}{\lambda p_b + \lambda p_c + p_e}\cdot
\frac{\lambda p_b}{\lambda p_b + \mu p_e}
\end{equation}

where this likelihood function reflects the mutual support for
equivalence class $\left\lbrace d,e\right\rbrace$.  Note that the
final term reflects the fact that competitors $b$ and $e$ have their
support active when vying for fourth place, as members of both their
teams have finished at this point.

This probability model could easily be modified to account for
specific circumstances.  The cheering effect could be asymmetrical
(with $a$ helping $b$ but not vice-versa, for example).  The effect
might operate only on certain ordered pairs, following a directed
graph (cyclic or acyclic) instead of equivalence classes.  Or perhaps
the effect might have a finite lifetime: if $a$ places
$n^\mathrm{th}$, then the cheering effect is active only for
competitors placing $(n+r)^\mathrm{th}$ or above, for some fixed $r$.
There is no reason that $\lambda$ could not be less than one, which
would indicate that some competitors inhibit, rather than help,
others.

The probability model of Figure~\ref{tikzabcde} is applied, in a
simple one-parameter form, to two datasets: one a set of preferences
of different types of sushi, and one in the field of e-sports.

\section{Red bus, blue bus}

The classic red bus-blue bus problem was originally stated by
\cite{mcfadden1980} although different metaphors have been employed
for the same phenomenon by \cite{tverskey1972}.  The canonical problem
is as follows.  A commuter has a choice of going to work by different
transport methods.  Here, we can assume that he has possibly
strong---but unknown---views on different forms of transport which
might incorporate a random component.  If considering whether to use a
car or a bus, the colour of the bus is of no importance to him.  Such
preferences are problematic for Bradley-Terry: If our subject is
indifferent between cars and buses, then $p_{C} = p_{RB}$.  If he is
also indifferent to colour of bus, so $p_{RB}=p_{BB}$, the following
paradox occurs.  Bradley-Terry strengths must be equal so $p_{C} =
p_{RB} = p_{BB}=\frac{1}{3}$.  Thus $\Prob(C| \left\lbrace
C,RB\right\rbrace)=\frac{1}{2}$.  Now we introduce a blue bus as an
option.  Any realistic choice model will have $\Prob(C | \left\lbrace
C,RB,BB\right\rbrace)=\frac{1}{2}$ but Bradley-Terry gives us
$\frac{1}{3}$.  Various solutions to this problem have been proposed.
\cite{tverskey1972}, for example, presents an approach involving
overlapping ``aspects'' of the elements of choice set;
\cite{mcfadden1980} suggests using a generalized extreme-value model.

Here I sugest an alternative in the form of a generalization of
Plackett-Luce likelihood functions for ranking options, using the
multiplicative generalization to Bradley-Terry strengths presented
above.  If we ask our respondent to rank his options, it is highly
probable that he will put RB and BB consecutively (because they are
essentially indistinguishable).  Can we quantify the strength of this
effect?  To do this, we define a bespoke function \code{RB\_BB\_LF()}
which returns a \code{hyper3} log-likelihood function corresponding to
repeated observations of our commuter's reported ranks for the five
options:

<<defineredbuslf>>=
`RB_BB_LF` <- function(lambda){
    ec <- c(C=1, T=2, RB=3, BB=3, W=4) 
    h <- c(1, 1, lambda, 1)           
    (
        cheering3(v=c("RB", "BB", "C" , "T", "W"), e=ec, h=h)*3 + 
        cheering3(v=c("BB", "RB", "T" , "C", "W"), e=ec, h=h)*2 + 
        cheering3(v=c("T" , "BB", "RB", "C", "W"), e=ec, h=h)*5 + 
        cheering3(v=c("W" , "BB", "RB", "T", "C"), e=ec, h=h)*4 + 
        cheering3(v=c("C" , "RB", "BB", "W", "T"), e=ec, h=h)*3 + 
        cheering3(v=c("BB", "C" , "RB", "T", "W"), e=ec, h=h)*3
    )
}
@

Above, we see from the function body that he reported $RB\succ BB\succ
C\succ T\succ W$ three times [first row], and so on; perhaps his
ranking depends on the weather or how tired he is on any given day.
Vector \code{ec} places the options into equivalence classes;
equivalence class 3 comprises the two buses and the other classes are
unsupported as they are of size one.  Vector \code{h} encodes the
strength of the nonindependence.  Observe that in almost every case he
ranks RB and BB consecutively.  Function \code{RB_BB_LF()} takes
argument \code{lambda} that quantifies the perceived similarity
between RB and BB in the same way as function \code{oldfirm()} above.

<<definefunctiono>>=
o <- function(lambda){maxp(RB_BB_LF(lambda), give=1)$`Log-likelihood`} 
@ 

Above, we define function \code{o()} which returns the support for a
given value of $\lambda$ at the evaluate [over permissible Bradley
  Terry strengths for the options].  We can treat \code{o()} as a
support function for $\lambda$, and then maximize it:

<<label=maxlikesim, cache=TRUE>>=
(osup <- optimize(o, c(10, 40), maximum=TRUE))
@ 

So a likelihood ratio test of the null that $\lambda=1$ would be:

<<label=LRT_RB, cache=TRUE>>=
(suppdiff <- osup$objective - o(lambda = 1))
@

We can thus improve the hypothesis that $\lambda=1$, corresponding to
an unaltered Plackett-Luce system, by over 5.6 units of support by
dint of choosing instead $\lambda = 10.49$.  This easily satisfies
\citeauthor{edwards1992}'s two-units-of-support criterion;
alternatively, we could observe that this observation lies in the tail
region of its asymptotic $\chi^2_1$ distribution (Wilks's theorem),
with a $p$-value of \Sexpr{signif(pchisq(suppdiff*2, df=1,
  lower.tail=FALSE),3)}.

%It is straightforward to find a profile
%support function for $\lambda$, Figure~\ref{profilesupportlambdaRBBB}.
%We see that support of $\log\lambda$ is closely quadratic over a broad
%range.

%\begin{figure}[htbp]
%\begin{center}
<<plotproflike, echo=FALSE>>=
if(FALSE){
    lambda <- exp(seq(from=log(0.9), to=log(67), len=19)) 
    L <- sapply(lambda, o)
    plot(log(lambda), L-max(L), type="b")
    abline(h=c(0, -2))
    abline(v=0)
}
@ 
%\caption{A\label{profilesupportlambdaRBBB} profile support function
%  for the red bus, blue bus parameter $\lambda$, synthetic dataset.
%  Vertical line indicates null of $\lambda=1$, horizontal lines show
%  maximum support and $-2$}
%\end{center}
%\end{figure}

\subsection{Sushi}

We now apply the ideas above to a dataset obtained from the {\tt
  preflib} library~\citep{MaWa13a} in which ten different types of
sushi were placed in order of preference by 5000 judges.  Three of the
sushis (maguro, toro, tekka-maki) were made with tuna, and it is
reasonable to view these three types as being exchangeable in much the
same way as the red and blue buses.  To proceed, we need to define
equivalence classes of sushi---one for tuna-based sushis and one for
everything else.  Of course, one can also imagine the opposite
phenomenon whereby the judge decides he has had enough tuna for the
moment and this disincentivises further tuna choices.  This mechanism
would be modelled by having $\lambda<1$.

Implementation in \code{hyper3} formalism is straightforward.  The
dataset used is \code{sushi\_table}, part of the \code{hyper2} package:

<<showsushitable>>=
noquote(head(sushi_table))
@

Thus, from the top line, we see that the first judge placed tamago
first, followed by anago and ika.  The equivalence classes are given
by

<<show_sushi_eq_classes>>=
sushi_eq_classes
@

so class 2 is the tuna group and class 1 everything else.  We can now
define a bespoke likelihood function for these observations, using
$\lambda$ as the argument:

<<defcalculate_sushi_H>>=
make_sushi_H <- function(lambda){
    H <- hyper3()
    for(i in seq_len(nrow(sushi_table))){
        H <- H + cheering3(sushi_table[i,], e=sushi_eq_classes, h=c(1, lambda))
    }
    return(H)
}
@

Then, as before, we may define a profile likelihood function of
$\lambda$, Figure~\ref{profilesupportsushi}.

<<calclikesushi,echo=FALSE, cache=TRUE>>=
f <- function(lambda){
   H <- make_sushi_H(lambda)
   maxp(H,give=1)$`Log-likelihood`
}
s <- exp(seq(from = log(0.9), to = log(10), len = 23))
L <- sapply(s,f)
@ 

<<findstuffsushi,echo=FALSE,cache=TRUE>>=
osushi <- optimize(f, c(2,4), maximum=TRUE)
@ 

<<tinysushi,echo=FALSE>>=
sushi_maxlambda <- osushi$maximum
@ 

<<findothersushistuff, echo=FALSE, cache=TRUE>>=
g <- function(lambda){f(lambda) - osushi$objective + 2}
sushi_lower <- uniroot(g, interval = exp(c(0,1)), tol = 0.01)$root
sushi_upper <- uniroot(g, interval = exp(c(1,2)), tol = 0.01)$root
@ 

\begin{figure}[htbp]
  \begin{center}
<<plotlikesushi, echo=FALSE>>=
plot(log(s),L-max(L),type='b',
     xlab=expression(log(lambda)), ylab='Log-likelihood')
abline(v=0,col='gray')
abline(h=c(0,-2),col='gray')
segments(x0 = log(sushi_maxlambda), y0 =  0.1, y1 = -1.0, col = 'red' )
segments(x0 = log(sushi_lower    ), y0 = -2.2, y1 = -1.8, col = 'blue')
segments(x0 = log(sushi_upper    ), y0 = -2.2, y1 = -1.8, col = 'blue')
@ 
\caption{A\label{profilesupportsushi} profile support function for the
  exchangeability term $\lambda$ in the case of sushi preferences.
  Vertical gray line shows $H_0\colon\lambda =1$, horizontal lines
  show the two-units-of-support limits with the credible interval of
  $(\Sexpr{round(sushi_lower,2)},\Sexpr{round(sushi_upper,2)})$,
  marked in blue.  Red bar shows evaluate at
  $\lambda\simeq\Sexpr{round(sushi_maxlambda,2)}$}
\end{center}
\end{figure}


\subsection{Counterstrike}

\newcommand{\skad}{{\sc Skadoodle}}
\newcommand{\rush}{{\sc RUSH}}
\newcommand{\tarik}{{\sc tarik}}
\newcommand{\stew}{{\sc Stewie2K}}

{\em Counter Strike: Global Offensive (CS:GO)} is a multiplayer
first-person shooter game in which two teams of five compete in an
immersive virtual reality combat environment.  Gameplay is typically
complex, including features such as cooperative teamwork and real-time
strategic fluidity.

Here I analyse a dataset taken from recent e-sports tournament {\em
  ELEAGUE Major: Boston 2018}.  This Valve-sponsored Major featured 24
elite teams competing for a million dollar prize pool; peak concurrent
viewership exceeded $1.3\times 10^6$, the highest in {\em CS:GO}
history at the time.  I study the performance of tournament champions
{\em Cloud9} who, as underdogs, narrowly defeated aggressive European
rivals {\em FaZe Clan} in the final.

Typically when a player is killed, the team suffers a number of
strategic disadvantages; player deaths are thus a quantifiable proxy
for both individual ability and also both teams' tactical execution.
Here I analyse the order statistic for number of deaths of each of the
five {\em Cloud9} players.

<<echo=FALSE>>=
a <- read.table("cloud9_boston_2018.txt", header=TRUE)
a[,-1]
@

From the first line we see that {\em Cloud9} played {\em FaZe Clan} on
the {\em inferno} map, won the match, and \skad\ had the least number
of deaths, followed by \rush, then \tarik, and so on.  Consider two
{\em Cloud9} players: \skad\ and \stew.  These teammates, although
comparable in skill [we fail to reject $H_0\colon p_{\mbox{\tiny\sc
      Skadoodle}} = p_{\mbox{\tiny\sc Stewie2k}}$, Plackett-Luce,
  $p=0.28$] have playing styles that stand in dramatic contrast.
Commentators characterise \skad\ as emphasising reactive map control
while \stew\ is more of a disruptive presence engaging in
high-variance play.

Given that these two players offer equally attractive and accessible
targets to the opposing team, it is possible that some tactical
situations might result in a systematic preference for targetting
\skad, and some a preference for killing \stew.  If this is so, the
deaths will be overdispersed and one would have a red bus-blue bus
situation, but with $\lambda<1$.

<<defredbusfunc, echo=FALSE>>=
mk <- function(lam){
    e <- c(Skadoodle=2, RUSH=1, tarik=1, autimatic=1, Stewie2K=2)
    H3 <- hyper3()
    for(i in seq_len(nrow(a))){
        d <- strsplit(a$deathorder[i],", ")[[1]]
        H3 <- H3 + cheering3(d, e=e, help=c(1, lam))
    }
    return(H3)
}
f <- function(l){maxp(mk(l), give=1)$`Log-likelihood`}
@

<<csgo_findopt, cache=TRUE, echo=FALSE>>=
csgo_optimize <- optimize(f,c(0.09, 0.14), maximum = TRUE, tol = 0.01)
csgo_optlam   <- csgo_optimize$maximum
csgo_support  <- csgo_optimize$objective - f(1)
csgo_pvalue   <- pchisq(csgo_support, df = 1, lower.tail = FALSE)
@


<<csgo_calcklikelambda, cache=TRUE, echo=FALSE>>=
lam <- exp(seq(from = -4, to = log(1.1), len = 23))
L <- sapply(lam, f)
@

<<cgso_findinterval, cache=TRUE, echo=FALSE>>=
g <- function(l){f(l) - csgo_optimize$objective  + 2}
csgo_lower <- uniroot(g, interval = exp(c(-4,-3)))$root
csgo_upper <- uniroot(g, interval = exp(c(-1,-0)))$root
@ 

\begin{figure}[htbp]
  \begin{center}
<<csgo_plotlikelam, echo=FALSE>>=
plot(log(lam), L - max(L), type='b')
segments(x0 = log(csgo_optlam),y0 = -0.5, y1 =  0.1, col='red')
segments(x0 = log(csgo_lower), y0 = -2.2, y1 = -1.8, col = 'blue')
segments(x0 = log(csgo_upper), y0 = -2.2, y1 = -1.8, col = 'blue')
abline(v = 0, col = 'gray')
abline(h = c(0, -2), col = 'gray')
@ 
\caption{A\label{profsupcounterstrike} profile support function for
  the red bus-blue bus parameter $\log\lambda$ applied to mutually
  supporting equivalence class
  $\left\lbrace\mbox{\skad},\mbox{\stew}\right\rbrace$.  Vertical gray
  line shows $H_0\colon\lambda =1$, for which the support is about
  $\Sexpr{round(csgo_support,2)}$ relative to the evaluate
  $\hat{\lambda}\simeq\Sexpr{round(csgo_optlam,3)}$ (small red bar),
  corresponding to a $p$-value of about
  $\Sexpr{round(csgo_pvalue,3)}$.  Horizontal lines show the
  two-units-of-support credible interval of
  $(\Sexpr{round(csgo_lower,3)},\Sexpr{round(csgo_upper,3)})$, marked
  in blue}
\end{center}
\end{figure}

To assess this, we follow exactly the same protocol as for the sushi
preference dataset but using the deathorder statistic from the dataset
above.  We specify two equivalence classes: one comprising \skad\ and
\stew, and one everyone else.  Figure~\ref{profsupcounterstrike} gives
a support function for $\lambda$ showing, among other things, that we
may reject the hypothesis that $\lambda=1$ in favour of $\lambda<1$ by
a likelihood ratio of about $\Sexpr{round(csgo_support,2)}$.  We
observe that this exceeds the two units of support criterion of
\cite{edwards1992} [asymptotic $p$-value
  $\simeq\Sexpr{round(csgo_pvalue,3)}$] so we may infer that there is
indeed some systematic mechanism---perhaps a positive feedback
loop---which tends to magnify differences in death statistic.

\section{Conclusions and further work}

Bradley-Terry strengths $p_1,\ldots,p_n$, $p_i\geqslant 0$, $\sum
p_i=1$ are generalized with non-negative multiplicative $\lambda$-
factors that represent temporary or ephemeral changes in competitive
strength.  The {\tt hyper2} R package implements the resulting
likelihood functions, and furnishes a wide range of functionality
geared towards assessing meaningful nulls using likelihood methods.

The standard Plackett-Luce likelihood function
$\prod_{i=1}^{n}p_{(i)}\left[\sum_{j=i}^n p_{(j)}\right]^{-1}$ over
Bradley-Terry strengths assumes conditional independence of successive
rank observations $p_{(1)},\ldots p_{(n)}$, an assumption which is
broken in the current work.  As indicated above, multiplicative
$\lambda$- factors may be used to implement many different
non-independence schemes and comparing these might be interesting.

Further work might include an individualised multiplicative draw
factor, giving rise to likelihood functions such as
$\frac{p_1^{w_1};(\lambda_1p_1+\lambda_2p_2)^{d_{12}};
  p_2^{w_2}}{\left(p_1(1+\lambda_1)+p_2(1+\lambda_2)\right)^{w_1+d_{12}+w_2}}$.
One might also consider multiplicative factors for the strengths of
{\em reified} entities.  These modifications would be directly
representable with existing {\tt hyper3} idiom.



\bibliography{hyper2}
\end{document}
