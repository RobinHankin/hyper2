---
title: "Very simplified likelihood"
author: "Robin Hankin"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("hyper2")
library("pracma")
library("magrittr")
```

```{r hexsticker, out.width='20%', out.extra='style="float:right; padding:10px"',echo=FALSE}
knitr::include_graphics(system.file("help/figures/hyper2.png", package = "hyper2"))
```

Consider a race between a competitor of Bradley-Terry strength $a$ and
$n-1$ competitors each of strength $b$; we require $a+b=1$.  NB we
have $n$ competitors in total, 1 of strength $a$ and $n-1$ of strength
$b$.  An observation is indexed by $r$, the rank of the focal
competitor: we have $1\leqslant r\leqslant n$.  The initial field
strength is $a+(n-1)b=1+b(n-2)b$.

Suppose our bro comes first (zero $b$'s ahead of him) [$r=1$]:

\[
\mathcal{L}(a)=\frac{a}{a+(n-1)b}=\frac{1-b}{1+(n-2)b}
\]

Comes second [$r=2$], one $b$ ahead of him:

\[
\mathcal{L}(a)=\frac{b}{a+(n-1)b}\cdot\frac{a}{a+(n-2)b}=
\frac{b(1-b)}{(1+(n-2)b)\cdot(1+(n-3)b)}
\]


Third [$r=3$]; two ahead of him:

\begin{eqnarray}
\mathcal{L}(a) &=&
\frac{b}{a+ n   b}\cdot
\frac{b}{a+(n-1)b}\cdot
\frac{a}{a+(n-2)b}\\
&=&
\frac{b^2(1-b)\rule{0mm}{10mm}
}{
(1+(n-2)b)\cdot(1+(n-3)b)\cdot(1+(n-4)b)}
\end{eqnarray}


Fourth [$r=4$], three clones finish before the focal competitor:

\[
\mathcal{L}(a)=
\frac{b^3(1-b)
}{
(1+(n-2)b)\cdot(1+(n-3)b)\cdot(1+(n-4)b)\cdot(1+(n-5)b)}
\]



Placing with general rank $r$, $1\leqslant r\leqslant n$.  Rank $r$
means $r-1$ clones finish ahead of the focal competitor:

\begin{eqnarray}
\mathcal{L}(a) &=&
\frac{b^{r-1}(1-b)
}{
\underbrace{(1+(n-2)b)\cdot(1+(n-3)b)\cdots(1+(n-r-1)b)}_{\mbox{$r$ terms}}
}
\\&=&
\frac{B-1\rule{1mm}{10mm}}{(n+B-2)\cdot(n+B-3)\cdots(n+B-r-1)}
\end{eqnarray}

where $B=1/b$.  Recall that $r$ is the rank and $r-1$ clones finish
ahead of the FC.  This is

\[
\mathcal{L}_{r,n}(a)=\frac{(B-1)(n+B-r-2)!}{(n+B-2)!},\qquad B=1/(1-a)
\]

\[
=\frac{(B-1)\Gamma(n+B-r-1)}{\Gamma(n+B-1)},\qquad B=1/(1-a)
\]

Now some Mathematica:

```
f[B_, n_, r_] := 
 Log[(B - 1)] + Log[Gamma[n + B - r - 1]/Gamma[n + B - 1]]
Assuming[B > 1 && p > 0 && p < 1, 
 Series[f[B, n, n*p], {n, Infinity, 1}]]
```

gives 


\[
\mathcal{S}=\log\mathcal{L}_{r,n}(a)=\log(B-1) + (B-3/2)\log(1-r/n) + 
\mathcal{O}\left(n^{-1}\right)
\]

(where $K$ is an arbitrary constant).  Here, $\mathcal{S}$ is the
support (log-likelihood).  Observing that
$\partial^2\mathcal{S}/\partial B^2=-(B-1)^{-2}<0$, the evaluate would
be unique.


# R idiom


```{r, label=definefsingle}
f_single <- function(a, r, n) {  # not vectorised
    stopifnot(r>0)
B <- 1/(1-a)
  log(B-1) + lgamma(n+B-r-1) - lgamma(n+B-1)
}

f_vec_a <- function(a,r,n){  # vectorised in 'a' but not in 'R'
	sapply(a,function(a){f_single(a,r=r,n=n)})
}

f_vec_arn <- function(a,r,n){
	## vectorized in 'a'; treats 'r' and 'n' as
	## vectors of independent observations

	M <- cbind(r,n)

    out <- 0
    for(i in seq_len(nrow(M))){out <- out + f_vec_a(a,r=M[i,1],n=M[i,2])}
	return(out)
}

fapprox <- function(a,r,n){
    stop("not written")
}
```

```{r label=defdiff}
fdash <- function(a,r,n){  # dS/da = dS/dB*dB/da
	B <- 1/(1-a)
    (1/(B-1) + psigamma(n+B-r-1) - psigamma(n+B-1))*B^2
}
```

Now verify `fdash()`:

```{r label=verifydiff}
a <- 0.34
d <- 1e-3
r <- 4
n <- 10
c(
  numerical  = (f_single(a+d/2, r, n) - f_single(a-d/2, r, n))/d,
  analytical = fdash(a,r,n)
)

```

Now use the derivative to find the maximum likelihood point

```{r,label=defmle}
MLE <- function(r,n,give=FALSE){
	d <- 1e-6
	out <- optimize(f_vec_arn, c(d, 1-d), r=r, n=n, maximum=TRUE)
	if(!give){out <- out$maximum}
	return(out)
}

MLEa <- function(r,n,give=FALSE,small=1e-4){
	out <- uniroot(f=function(a){fdash(a,r,n)},interval=c(small,1-small))
	if(!give){out <- out$root}
	return(out)
}

showdiff <- function(x,y){c(way1=x,way2=y,diff=x-y,logratio=log(x/y))}

rbind(
showdiff(MLE(3,9),MLEa(3,9)),
showdiff(MLE(3,7),MLEa(3,7))
)
```


Now verification

```{r,label=verifytwo}
a <- 0.23423434
b <- 1-a
B <- 1/b
n <- 9  # 9 runners
r <- 4  # FC came 4th
c(
way1 = b^3*a/((a+9*b) * (a+8*b) * (a+7*b) * (a+6*b)),
way2 = (B-1)/prod(B+(5:8)),
way3 = f_vec_arn(a,r,n)
)
```


```{r,label=makejj}
out <- list()
for(n in 1:10){
  out[[n]]  <- rep(NA,n+1)
  for(r in seq_len(n+1)){
    if(r==1){
      jj <- 1
    } else if(r == n+1){
      jj <- 0
    } else {
      jj <- MLE(r,n)
    }
    out[[n]][r] <- jj
  }
}
out
```

```{r,label=plotjj}
plotterp <- function(...){
plot(NA,xlim=c(2,10),ylim=c(0,1),type="n",xlab="n",ylab=expression(hat(a)))
for(n in 2:10){
  for(r in seq_len(n)){ points(n,out[[n]][r],pch=16,col='black') }
  for(r in seq_len(n)){ text(n+0.2,out[[n]][r],r,cex=0.5,col='gray') }
}
}
plotterp()
pdf(file="dotprobs.pdf")
plotterp()
dev.off()
```


# Note on the differences between probability and likelihood

The likelihood calculations above can be confusing becuase likelihood
is not the same as probability.  Consider, for example, $n=3$ so we
have 3 competitors: the focal competitor of strength $a$ and two
clones each of strength $b=1-a$.

probability of focal competitor coming first [i.e. $r=1$]:

\[\frac{a}{a+2b}=\frac{1-b}{1+b}\]

Now what is the probability of coming second, that is, $r=2$?

My first thought was:

\[
\mbox{prob of coming second}   =
\frac{b}{a+2b}\cdot\frac{a}{a+b}=\frac{b(1-b)}{1+b}\qquad{\mbox{WARNING}}
\]

but this is *incorrect*: there are two clones of strength $b$, and the
likelihood arguments above do not distinguish between their finishing
order.  We can name the competitors $a$, $b_1$ and $b_2$ and specify
that $b_1$ and $b_2$ both have strength $b$.  Then "the focal
competitor came second" means that the finishing order was $b_1\succ
a\succ b_2$ or $b_2\succ a\succ b_1$.  Noting that these events are
disjoint, the probability would be

\[\mbox{prob of coming second}   =
\operatorname{P}(b_1\succ a\succ b_2)      +
\operatorname{P}(b_2\succ a\succ b_1)      =
\frac{b}{a+2b}\cdot\frac{a}{a+b} +
\frac{b}{a+2b}\cdot\frac{a}{a+b} = \frac{2(1-b)}{1+b}
\]

[see the factor of two difference].  Similarly for the focal
competitor coming last we need to sum the probabilities of finishing
orders $b_1\succ b_2\succ a$ and $b_2\succ b_1\succ a$:

\[\mbox{prob of coming third}=
\operatorname{P}(b_1\succ b_2\succ a) +
\operatorname{P}(b_2\succ b_1\succ a) =
\frac{b}{a+2b}\cdot\frac{b}{a+b}+
\frac{b}{a+2b}\cdot\frac{b}{a+b}=
\frac{2b^2}{1+b}.
\]

Above we see a factor of two difference between the probabilities and
the likelihoods presented previously.  Note that there is no such
factor for the probability of the focal competitor finishing first:

\[
\mbox{prob of coming first}      =
\operatorname{P}(a\succ b_1\succ b_2)      +
\operatorname{P}(a\succ b_2\succ b_1)      =
\frac{b}{a+2b}\cdot\frac{b}{b+b} +
\frac{b}{a+2b}\cdot\frac{b}{b+b} = \frac{1-b}{1+b}
\]


Now we see that the probabilities sum to

\[
\frac{1-b}{1+b} + \frac{2b^2}{1+b} + \frac{2(1-b)}{1+b}=1
\]


Now try $n=4$ [i.e. 4 competitors: the FC and three clones].  Taking
things one step at a time:

\[
\mbox{prob of coming first} =
3!\cdot\frac{a}{a+3b}\cdot\frac{b}{3b}\cdot\frac{b}{2b}\cdot\frac{b}{b}=
\frac{1-b}{1+2b}
\]

\[
\mbox{prob of coming second} =
3!\cdot\frac{b}{a+3b}\cdot\frac{a}{a+2b}\cdot\frac{b}{2b}\cdot\frac{b}{b}=
3\cdot\frac{b(1-b)}{(1+2b)(1+b)}
\]

\[
\mbox{prob of coming third} =
3!\cdot\frac{b}{a+3b}\cdot\frac{b}{a+2b}\cdot\frac{a}{a+b}\cdot\frac{b}{b}
=
6\cdot\frac{b^2(1-b)}{(1+2b)(1+b)}
\]
\[
\mbox{prob of coming fourth (last)} =
3!\cdot\frac{b}{a+3b}\cdot\frac{b}{a+2b}\cdot\frac{b}{a+b}\cdot\frac{a}{a}
=
6\cdot\frac{b^3}{(1+2b)(1+b)}
\]

The sum would be $[(1+b)(1-b) + 3b(1-b) + 6b^2(1-b) +
6b^3]/[(1+b)(1+2b)]=1$.

Now consider the general case.  This time we consider $n$ clones of
strength $b$ and one of strength $a=1-b$, $n+1$ competitors.

\[
\mbox{prob of coming first} =
n!\cdot\frac{b}{a+nb}\cdot\frac{b}{nb}\cdot\frac{b}{(n-1)b}\cdots\frac{b}{b}
=
\frac{b}{1+(n-1)b}
\]

\[
\mbox{prob of coming second} =
n!\cdot\frac{b}{a+nb}\cdot\frac{a}{a+(n-1)b}\cdot\frac{b}{(n-1)b}\cdot\frac{b}{(n-2)b}\cdots\frac{b}{2b}\cdot\frac{b}{b}
=n\cdot X
\]

\[
\mbox{prob of coming third} =
\frac{b}{a+nb}\cdot
\frac{b}{a+(n-1)b}\cdot
\frac{a}{a+(n-2)b}\cdot
\frac{b}{(n-2)b}\cdots
\frac{b}{2b}\cdot\frac{b}{b}
=\frac{n!}{(n-2)!}\cdot X
\]

Now consider the case where there are $p$ clones coming ahead of the
focal competitor:


\[
\mbox{prob of having $r$ clones ahead}
=\frac{n!}{(n-p)!}\cdot X
\]



## Visuals


Now some likelihood visuals.  Suppose $n=9$:

```{r,label=likevis,cache=F}
plotter <- function(...){
a <- seq(from=0,to=1,by=0.01)
n <- 9 # the FC and 8 clones
jj <- exp(f_vec_arn(a,3,n))
plot(a,jj/max(jj,na.rm=TRUE),type='n',xlab=expression(a),ylab="likelihood")
grid()
rain <- rainbow(n)
for(r in seq_len(n)){
  y <- exp(f_vec_a(a,r,n))
  if(r==1){y[length(y)] <- 1}
  y <- y/max(y,na.rm=TRUE)
  if(r>1){y[length(y)] <- 0}
  points(a,y,type='l',lwd=4,col=rain[r])
}
abline(v=MLE(r=5,n=9))
text(0.09,0.95,"r=9",col=rain[9])
text(0.18,0.93,"r=8",col=rain[8])
text(0.25,0.90,"r=7",col=rain[7])
text(0.29,0.83,"r=6",col=rain[6])
text(0.33,0.76,"r=5",col=rain[5])
text(0.35,0.65,"r=4",col=rain[4])
text(0.39,0.54,"r=3",col=rain[3])
text(0.45,0.41,"r=2",col=rain[2])
text(0.59,0.21,"r=1",col=rain[1])
}

plotter()
pdf(file="ninelikes.pdf")
plotter()
dev.off()
```


```{r shown4n8mle}
MLE(r=5,n=9)
```


```{r usehyper3,cache=TRUE}
n <- 7
a <- seq(from=0,to=1,by=0.01)
fhyper3 <- function(r,n){
  out <- hyper3()
  out['a'] <- 1
  out['b'] <- r
  for(i in (n-r):n){
    out[c(a=1,b=i)] %<>% dec
  }
  return(out)
}

plot(NA,xlim=c(0,1),ylim=c(0,1),type='n')
M <- cbind(a=a,b=1-a)
for(r in 0:7){
  y <- loglik(M,fhyper3(r,n),log=FALSE)
  y <- y/max(y, na.rm=TRUE)
points(M[,1],y,type="l",lwd=8)
}
```





# Formula 1: Ocon vs Gasly

```{r maxvscheco}
a <- read.table("formula1_2023.txt",header=TRUE)
a <- a[,seq_len(ncol(a)-1)]
d_ocon  <- as.numeric(as.matrix(a)["Ocon",])
d_gasly <- as.numeric(as.matrix(a)["Gasly",])
d_ocon [is.na(d_ocon )] <- 22
d_gasly[is.na(d_gasly)] <- 22
d_ocon
d_gasly
MLE(d_ocon ,22)
MLE(d_gasly,22)
```

two sample test:

```{r 2samptest}
both <- c(d_ocon,d_gasly)
bothmax <- MLE(both,22)

d_ocon_max  <- MLE(d_ocon ,22)
d_gasly_max <- MLE(d_gasly,22)
f_vec_arn(bothmax,d_ocon ,22)
f_vec_arn(bothmax,d_gasly,22)
f_vec_arn(d_ocon_max ,d_ocon ,22)
f_vec_arn(d_gasly_max,d_gasly,22)

Lambda <- (
   (f_vec_arn(d_gasly_max  ,d_ocon,22)+f_vec_arn(d_gasly_max  ,d_gasly,22)) -
   (f_vec_arn(bothmax      ,d_ocon,22)+f_vec_arn(bothmax      ,d_gasly,22))
)
Lambda
pchisq(-2*Lambda,df=1,lower.tail=FALSE)
```

# Formula 1: Formula 1 drivers: career arc

```{r getnumgetfoc}
readstring <- function(year){read.table(paste("formula1_",year,".txt",sep=""))}
getfoc <- function(year,comp="Perez"){  # get focal competitor
  M <- as.matrix(readstring(year))
  out <- suppressWarnings(as.numeric(M[comp,seq_len(ncol(M)-1)]))
  out[is.na(out)] <- nrow(M)
  return(out)
}
getnum <- function(year){nrow(readstring(year))} # number of competitors

perez <- lapply(2011:2023,getfoc)
print(perez)
y <- unlist(lapply(seq_along(perez),function(i){mean(perez[[i]])}))
x <- 2011:2023
plot(x,y)
summary(lm(y~x))
```


```{r checofirst}
checo_like <- function(a){
  out <- a*0
  for(year in 2011:2023){ out <- out + f_vec_arn(a,getfoc(year),getnum(year)) }
  return(out)
}

a <- seq(from=0.45,to=0.62,by=0.01)
cL <- checo_like(a)
cL <- cL - max(cL)
plot(a,cL,type='b')
abline(h=c(0,-2))
```


```{r definef1log}
f1_logistic <- function(vec){
  alpha <- vec[1]
  beta  <- vec[2]
  out <- 0
  for(year in 2011:2023){
     LO <- alpha + beta*(year-2011)
     strength <- exp(LO)/(1+exp(LO))
     out <- out + f_vec_arn(strength,getfoc(year),getnum(year))
  }
  return(out)
}
```


```{r calculatelogisticcontou,cache=TRUE}
a <- seq(from=-0.8,to=0.0,by=0.01)
b <- seq(from=0,to=0.2,by=0.01)
jj <- as.matrix(expand.grid(a,b))
L <- apply(jj,1,f1_logistic)
L <- L-max(L)
L <- matrix(L,length(a),length(b))
```

```{r showfilledcwithdot}
L <- pmax(L,-40)
showchec <- function(...){
contour(a,b,L,xlab=expression(alpha),ylab=expression(beta),levels=-c(2,5*(1:5)))
points(-0.27,0.0813,pch=16,cex=2)
}
showchec()
pdf(file="showchecolike.pdf")
showchec(a,b,L)
dev.off()
```

```{r uselogf1}
f1_logistic(c(0,0))
f1_logistic(c(0,0.0001))
f1_logistic(c(0.001,0))
```

```{r optimizelof1,cache=TRUE}
o <- optim(c(-0.2,0.1),fn=f1_logistic, control=list(fnscale = -1),hessian=TRUE)
```

```{r showjj}
o
o$par
o$hessian
eigen(o$hessian)
best <- o$par
f1_logistic(best)


jj <- best
jj[1] <- jj[1]*1.01
f1_logistic(jj) - f1_logistic(best)

jj <- best
jj[1] <- jj[1]*0.99
f1_logistic(jj) - f1_logistic(best)

jj <- best
jj[2] <- jj[2]*1.00001
f1_logistic(jj) - f1_logistic(best)

jj <- best
jj[2] <- jj[2]*0.99
f1_logistic(jj) - f1_logistic(best)
```


```{r testlogf1,cache=TRUE}
o_free <- optim(c(-0.2,0.1),fn=f1_logistic, control=list(fnscale = -1),hessian=TRUE)
o_constrained <- optim(c(-0.2,0.1),fn=function(v){
v[2] <- 0
return(f1_logistic(v))}, control=list(fnscale = -1),hessian=TRUE)
```

```{r showpvalue}
o_free
o_constrained
pchisq(2*(o_free$value - o_constrained$value),df=1,lower.tail=FALSE)
```


# in silico formula

```{r insn}
table(replicate(100,which(rrace3(pn=c(a=1,b=10),ps=c(a=0.9,b=0.1))=='a')))
```

```{r insilicoestmation,cache=TRUE}
n <- 1000
M <- matrix(0,n,4)
alpha_true <- 4
beta_true <- -6
for(i in seq_len(n)){
    x <- i/n
    LO <- alpha_true + beta_true*x
    p <- exp(LO)/(1+exp(LO))
    nn <- round(runif(1,10,100))
    M[i,1] <- x  # regressor
    M[i,2] <- which(rrace3(pn=c(a=1,b=nn),ps=c(a=p,b=1-p))=='a')
    M[i,3] <- nn+1
    M[i,4] <- p
}
head(M)
tail(M)
```

```{r definelikeinsilico}
likeinsilico <- function(vec,M){
  alpha <- vec[1]
  beta  <- vec[2]
  S <- 0
  for(i in seq_len(nrow(M))){
    x <- M[i,1]
    LO <- alpha + beta * x
    p <- exp(LO)/(1+exp(LO))
    S <- S + f_vec_arn(p,r=M[i,2],n=M[i,3]-1)
  }
return(S)
}
```


```{r testlikeininsilicco,cache=TRUE}
likeinsilico(c(4,-6),M)
likeinsilico(c(4,-6.5),M)
likeinsilico(c(4,-5.5),M)
```


```{r optimize_insilicco,cache=TRUE}
optim_ab <- optim(par=c(4,-6),likeinsilico,control=list(fnscale= -1),M=M)
optim_ab
```

```{r makeZcontourdataset,cache=TRUE}
a <- seq(from=3.8,to=4.2,by=0.01)
b <- seq(from=-6.4,to=-5.6,by=0.01)
Z <- as.matrix(expand.grid(a,b))
Z <- apply(Z,1,function(x){likeinsilico(x,M=M)})
Z <- matrix(Z,length(a),length(b))
```

```{r doZcontour}
Z <- Z-max(Z)
Z <- pmax(Z,-10)
contour(a,b,Z,levels=-c(0:10,15,20,30))
points(alpha_true,beta_true,pch=16,cex=4)
jj <- optim_ab$par
points(jj[1],jj[2],pch=4,cex=4,col='red')
grid()
#persp(a,b,Z)
```


# Mathematical Olympiad


```{r readoly}
sametest <- function(d1,d2,n){
  both <- c(d1,d2)
  bothmax <- MLE(both,n)

  d1max <- MLE(d1,n)
  d2max <- MLE(d2,n)
  Lambda <- (
  (f_vec_arn(d2max  ,d1,n) + f_vec_arn(d2max  ,d2,n)) -
  (f_vec_arn(bothmax,d1,n) + f_vec_arn(bothmax,d2,n))
)
  pchisq(-2*Lambda,df=1,lower.tail=FALSE)
}

a <- as.matrix(read.table("olympiad.txt"))
nrow(a)
d1 <- a["AUS",]
d2 <- a["NZL",]
d1
d2
sametest(d1,d2,100)
a["USA",]
a["CHN",]
sametest(a["USA",],a["CHN",],100)
sametest(a["USA",],a["CHN",],5)
```

# Athletics


```{r loadolymp}
O <- read.table("olympic_athletics_mens_100m.txt",header=TRUE)
head(O)
```

```{r analyze_athletics_olymp,cache=TRUE}
jjf <- function(x){f_vec_arn(x,O$rank,O$n)}
system.time(optolym <- optimize(jjf, c(0.45,0.65), maximum=TRUE))
```

```{r}
optolym
jjf(0.5)
jjf(optolym$maximum)
(LR <- 2*(jjf(optolym$maximum)-jjf(0.5)))
pchisq(LR,df=1,lower.tail=FALSE)
```

```{r make_athletic_like}
a <- seq(from=0.35,to=0.65,len=45)
L <- f_vec_arn(a,O$rank,O$n)
Lmax <- jjf(optolym$maximum)
Lmax
L <- L-Lmax
```

```{r findcredint}
(a_lower <- uniroot(function(x){jjf(x)+2-Lmax},interval=c(0.1,0.5))$root)
(a_upper <- uniroot(function(x){jjf(x)+2-Lmax},interval=c(0.5,0.9))$root)
```

```{r plotolymp}
plotolymp <- function(...){
plot(a,L,type='l',ylab="log-likelihood",lwd=2)
abline(h=c(0,-2))
ahat <- optolym$maximum
segments(x0=ahat,y0=-1.5,y1=0)
text(ahat,-0.91,expression(hat(a)==0.511),pos=2)
abline(v=0.5,lty=3)

segments(x0=a_lower,y0=-1.5,y1=-3.5)
segments(x0=a_upper,y0=-1.5,y1=-3.5)
grid()
text(x=a_lower,y=-3,"0.414",pos=4)
text(x=a_upper,y=-3,"0.602",pos=2)
}
plotolymp()
pdf(file="plotolymp.pdf")
plotolymp()
dev.off()
```
   

# Parkrun

```{r readresults}
results <- read.table("parkrun_results.txt",header=TRUE)
results
```

Above we see some parkrun results.  In parkrun number 148, I placed
229 out of a field of 318.

```{r calculate_parksupp,cache=TRUE}
S_parkrun <- function(a){f_vec_arn(a, r=results$place, results$runners)}
(optparkrun <- optimize(S_parkrun, c(0.2,0.8),maximum=TRUE))
```

```{r calcparka,cache=TRUE}
a <- seq(from=0.25,to=0.65,by=0.01)
S_parkrun_a <- S_parkrun(a) - optparkrun$objective
uniroot(function(a){S_parkrun(a) - optparkrun$objective+2},c(0.1,0.5))$root
uniroot(function(a){S_parkrun(a) - optparkrun$objective+2},c(0.5,0.9))$root
optparkrun$objective - S_parkrun(0.5)
```

```{r plotparkrunlike, fig.cap="Support curve for author's generalized Bradley-Terry strength.  Curve is normalised so that the evaluate of $\\hat{a}\\simeq 0.448$ has a support of zero.  Dotted line shows $\\mathcal{S}=-2$; intersection of this with the  curve shows that the support interval runs from about 0.315 to 0.567"}
plotparkrun <- function(...){
plot(a,S_parkrun_a,xlab="generalized Bradley-Terry strength",ylab="support",type='b',ylim=c(-5,0))
abline(h=c(0,-2),lty=2)
segments(x0=0.5,y0=-1.5,y1=0)
segments(x0=optparkrun$maximum,y0=-1,y1=0)
text(optparkrun$maximum,-0.8,expression(hat(a)==0.448),pos=2)
text(0.5,-1.5,expression(H[o]:a==0.5),pos=2)
}
plotparkrun()
pdf("plotparkrun.pdf")
plotparkrun()
dev.off()
```

Figure \@ref(fig:plotparkrunlike) shows a support curve for the
author's generalized Bradley-Terry strength, normalized so that
$\mathcal{S}(\hat{a})=0$.  We see a support interval [that is,
$\left\lbrace a\colon\mathcal{S}(a)\geqslant -2\right\rbrace$] of
about 0.315 to 0.567.  We might also observe that the support for
$H_0\colon a=\frac{1}{2}$ [that is, that the author has a 50\% chance
of running faster than a randomly chosen Parkrun competitor] is about
0.35, less than Edwards's two units of support criterion.  We thus see
some support for Hankin's assertion that he is "as fit as anyone
else there".

```{r calculatelikes1, cache=F}
a <- seq(from=0.005,to=0.85,by=0.01)
La <- f_vec_arn(a,r=results$place[1],results$runners[1])
La <- La - max(La,na.rm=TRUE)
plot(a,La,type='b',ylab='fishy')
abline(h=c(0,-2))
```

```{r splitresults}
supp2 <- function(vec,place,runners){
  a_stirling <- vec[1]
  a_auckland <- vec[2]
  out <- 0
  for(i in 1:12){
    out <- out + f_vec_a(a_stirling,r=place[i],runners[i])
  }
  for(i in 13:21){
    out <- out + f_vec_a(a_auckland,r=place[i],runners[i])
  }
  return(out)
}
```


```{r,label=calcbothlikes,cache=TRUE}
x <- seq(from=0.1,to=0.8,by=0.01)
jj <- as.matrix(expand.grid(x,x))
L <- apply(jj,1,supp2,results$place,results$runners)
L <- L - max(L,na.rm=TRUE)
L <- matrix(L,length(x),length(x))
```

```{r,label=plotbothlikes}
parkrun_contour <- function(...){
    par(pty='s')
    contour(x,x,L,asp=1,xlim=range(x),ylim=range(x),levels=-(0:5),xlab="2023",ylab="2024")
    abline(0,1)
}
parkrun_contour()
pdf(file="parkruncontour.pdf")
parkrun_contour()
dev.off()
```


```{r, label=onedimopt}
f <- function(a){supp2(c(a,a),results$place,results$runners)}
a <- seq(from=0.2,to=0.6,by=0.01)
alike <- sapply(a,f)
plot(a,alike-max(alike,na.rm=TRUE),type='b',ylab='blong')
abline(h=c(0,-2))
optimize(f,c(0.1,0.6),maximum=TRUE)
```

```{r, label=twodimopt}
f2 <- function(v){supp2(v,results$place,results$runners)}
optim(par=c(0.6,0.3),f2,control=list(fnscale=-1))
```

```{r,label=showchisquareasymptotic}
7380.314-7381.496
pchisq(2*(7381.496-7380.314),df=1,lower.tail=FALSE)
```

Above shows that there is no evidence to suggest that the Stirling
strength differs from the Auckland strength.  

## Fisher parkrun

```{r splitruns}
a <- read.table("parkrun_results.txt",header=TRUE)

rank_auck    <- a$place  [a$venue == "auckland"]
runners_auck <- a$runners[a$venue == "auckland"]

rank_stir    <- a$place  [a$venue == "stirling"]
runners_stir <- a$runners[a$venue == "stirling"]
```

```{r fisherparkrun}
M <- matrix(c(
    sum(rank_auck-1),sum(runners_auck-rank_auck),
    sum(rank_stir-1),sum(runners_stir-rank_stir)),2,2,byrow=TRUE)
dimnames(M) <- list(venue=c("Auckland","Stirling"),better=c(T,F))
M
fisher.test(M)
```

    
    

## Logistic parkrun

Now let's try a
logistic-style regression:


```{r logisticsupportparkrun,cache=FALSE}
results <- read.table("parkrun_results.txt",header=TRUE)
results
supp_logistic <- function(vec){
  alpha <- vec[1]
  beta <- vec[2]
  out <- 0
  for(i in seq_len(nrow(results))){
     LO <- alpha + beta*i
     strength <- exp(LO)/(1+exp(LO))
     out <- out + f_vec_arn(strength,results$place[i],results$runners[i])
  }
  return(out+13400)
}

c(
supp_logistic(c(0,0)),
supp_logistic(c(0,0.01)),
supp_logistic(c(0,-0.01))
)

```


```{r optimlogist, cache=FALSE}
optim(par=c(0,0),fn=supp_logistic,control=list(fnscale=-1))
optim(par=c(0,0),fn=function(v){supp_logistic(vec=c(v[1],0))},control=list(fnscale=-1))
```




## Education

The focal competitor

```{r focaleducation}
rank       <- c( 9,  7,  2, 3,  2,  8,  3,  4,  9)
class_size <- c(12, 17, 23, 9, 13, 14, 13, 12, 15)
course <- c("rings and modules","group theory","calculus","linear algebra",
"differential equations","topology","special relativity","fluid mechanics","Lie algebra")
category <- c("pure","pure","applied","applied","applied","pure","applied","applied","pure")
data.frame(course,category,rank,class_size)
```


```{r seppurefromapplied}
a <- seq(from=0.2,by=0.01,to=0.8)
wp <- category=='pure'
wa <- category=='applied'
```

```{r dofishereducation}
pure_betterthanme    <- sum(rank[wp]-1)
pure_worsethanme     <- sum(class_size[wp] - rank[wp])
applied_betterthanme <- sum(rank[wa]-1)
applied_worsethanme  <-  sum(class_size[wa] - rank[wa])
M <- matrix(c(pure_betterthanme,pure_worsethanme,applied_betterthanme,applied_worsethanme),2,2,byrow=TRUE)
dimnames(M) <- list(type=c("pure","applied"),beatsme=c(T,F))
M
fisher.test(M)
```

Pure first

```{r dopurefurst}
Lpure <- f_vec_arn(a,rank[wp],class_size[wp])
Lpure <- Lpure - max(Lpure,na.rm=TRUE)
plot(a,Lpure,ylab='pure strength')
optimize(function(a){f_vec_arn(a,rank[wp],class_size[wp])},c(0.3,0.9),maximum=TRUE)
optimize(function(a){f_vec_arn(a,rank[wa],class_size[wa])},c(0.3,0.9),maximum=TRUE)
```


```{r definesupp2ed}
supp2_ed <- function(vec,place1,place2,runners1,runners2){
  out <- 0
  M1 <- cbind(place1,runners1)
  M2 <- cbind(place2,runners2)
  for(i in seq_len(nrow(M1))){
    out <- out + f_vec_arn(vec[1],r=M1[i,1],n=M1[i,2])
  }
  for(i in seq_len(nrow(M2))){
    out <- out + f_vec_arn(vec[2],r=M2[i,1],n=M2[i,2])
  }
  return(out)
}
```

```{r,label=fishbish,cache=TRUE}
x <- seq(from=0.1,to=0.99,by=0.01)
jj <- as.matrix(expand.grid(x,x))
L <- apply(jj,1,supp2_ed,rank[wp],rank[wa],class_size[wp],class_size[wa])
L <- L - max(L,na.rm=TRUE)
L <- matrix(L,length(x),length(x))
```

```{r,label=contourstrength}
plotpureandapplied <- function(...){
par(pty='s')
contour(x,x,L,asp=1,xlim=range(x),ylim=range(x),levels=-(0:5),
xlab='pure strength',ylab='applied strength')
abline(0,1)
}
plotpureandapplied()
pdf(file="plotpureandapplied.pdf")
plotpureandapplied()
dev.off()
```

```{r,label=defsupp2}
jj1 <-
optim(
   c(.4,.8),
   function(vec){
      supp2_ed(vec,rank[wp],rank[wa],class_size[wp],class_size[wa])},
   control=list(fnscale = -1)
)
jj1
```

```{r,label=findmaxlike}
jj2 <-
optimize(function(v){supp2_ed(c(v,v),rank[wp],rank[wa],class_size[wp],class_size[wa])},
c(0.1,0.9),maximum=TRUE)
jj2
```

```{r,label=shochisw}
jj1$value
jj2$objective
jj1$value-jj2$objective
pchisq(2*(jj1$value-jj2$objective),lower.tail=FALSE,df=1)
```

