---
title: "Weakly informative observations: not the winner"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(hyper2)
```

# Partial rank observations: someone didn't win

(see also `inst/loser.Rmd`).
Here we consider a situation in which we make
an observation that is only very very weakly informative.  We consider
the observation ${\mathcal O}$ that a particular player *does not*
win: the order statistic is unobserved and we know nothing about it
except for the fact that (wlog) player 1 ($p_1$, aka "me") is not
first placed.  We consider whether there is evidence to support two
different assertions:

* $H_1\colon p_1=p_2=\ldots=p_n=\frac{1}{n}$
* $H_2\colon\exists i\neq 1 \mbox{ with } p_1\geq p_i>0$

Our alternative would be that $(p_1,\ldots,p_n)$ is unconstrained
except for the unit sum constraint:

* $H_0\colon\sum p_i=1$  (that is, no restriction)

(see how $H_2$ implies that someone is worse than me).  Observe that
$H_1$ and $H_2$ correspond to a restriction on allowable parameter
space.  To understand the informativeness of ${\mathcal O}$, we will
consider it in conjunction with some highly informative observations
${\mathcal I}$ which are order statistics:

```{r}
(H <- 
	suppfun(c(a=3,c=2,b=1,d=4)) + 
	suppfun(c(b=2,c=3,d=1    )) + 
	suppfun(c(d=2,a=3,c=1    )) + 
	suppfun(c(b=1,d=2        )) )
```
	
We will firstly consider these observations in isolation, then later
combine them with ${\mathcal O}$ which we will take to be
(conditionally) independent.  The first step is estimate the players'
strengths:

```{r}	
(mH0 <- maxp(H))
loglik(indep(mH0),H)
```

Above, `mH0` corresponds to an unconstrained optimization.  Note the
low but nonzero estimated strength of `p_1`.  Observe in passing that
we may reject $H_z\colon p_1=0$ unconditionally: we can deduce using
elementary arguments that the likelihood of $H_z$ is zero: if $p_1=0$,
then $p_4=0$ from the first line [because $p_4$ beat $p_1$ on that
occasion], and then $p_1=0$ from line 2; then $p_3=0$ from line 3,
$p_5=0$ from line 4 and finally $p_2=0$ from line 1.  Thus we can say
for sure that $H_z$ is incorrect and we may safely ignore it
henceforth.

## Hypothesis 1: equality


```{r}
equalp.test(H)
```

Thus there is no evidence to suggest that the players have different
strengths: we are unable to reject $H_1$, the hypothesis of equality.
Hypothesis $H_2$ is rather tricky so we will use the ersatz
alternative that $p_1\geqslant p_4$ (we choose $p_4$ on the grounds of
its low estimated value).  Then

```{r}
samep.test(H,c(1,4))
```

There is no reason to reject our null that $p_1\geqslant p_4$, for the
support falls below the two units of support criterion.  Further, see
how the constraint is active, for at the evaluate $p_1=p_4$ (the
numerical value of `small` is immaterial).

# Additional uninformative observation

Suppose we now make a perfectly uninformative observation: in an
independent trial, the order statistic for the five competitors was
one of the $5!=120$ possible orders.  How does this change our
relative support for $H_0$ against $H_1$ and $H_2$?  It should make no
difference.

```{r,uninformativesupportfunction}
library("partitions")
H # as before, here for convenience
M <- perms(5)   # uninformative!
SL <- list()
for(i in seq_len(ncol(M))){
	jj <- M[,i,drop=TRUE]
	names(jj) <- letters[seq_along(jj)]
	SL[[i]] <- H + suppfun(jj)
}
SL <- as.suplist(SL)
```

So `SL` is a compound object that gives a likelihood function for the
joint observation of the order statistics and the uninformative data a
subsequent (independent) trial was in any of the $5!=120$ possible
orders.  We can find the MLE for different observations:

```{r,mls0,cache=TRUE}
n <- 5 # number of players
small <- 1e-4
mSL0 <- maxplist(SL)
mSL1 <- maxplist(SL,startp=c(small*2,rep(small,n-2)),fcm=rbind(c(1,0,0,-1)),fcv=0)
mSL2 <- equalp(SL)
```

We use `like_single_list()` to calculate the likelihoods at the
different evaluates; note that this function returns likelihoods, not
supports.

```{r,supp2calc}
Supp2 <-
      log(c(
      H0 = like_single_list(indep(mSL0),SL),
      H1 = like_single_list(indep(mSL1),SL),
      H2 = like_single_list(indep(mSL2),SL)))
(Supp2 <- Supp2-max(Supp2))
```

which is identical to the previous support function for $H_1$ and
$H_2$ (apart from a traditional sign) taken from `equalp.test()` and
`mSL2` above.

